{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Descent Env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from descent_env import DescentEnv\n",
    "import random \n",
    "from q_learning_agent import QLearningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar render_mode a rgb_array para entrenar/testear\n",
    "from env_recorder_wrapper import VideoRecorderWrapper \n",
    "import wandb\n",
    "env = DescentEnv(render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de los estados\n",
    "\n",
    "**Nota:** es importante que chequeen el espacio de observación y el espacio de acción del entorno. Los números usados son ejemplos y pueden no ser correctos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altitude_space_start = 0\n",
    "altitude_space_end = 2.5\n",
    "altitude_space_num = 30\n",
    "altitude_space = np.linspace(altitude_space_start, altitude_space_end, altitude_space_num) \n",
    "\n",
    "vertical_velocity_space_start = -3\n",
    "vertical_velocity_space_end = 3\n",
    "vertical_velocity_space_num = 20\n",
    "vertical_velocity_space = np.linspace(vertical_velocity_space_start, vertical_velocity_space_end, vertical_velocity_space_num) \n",
    "\n",
    "target_altitude_space_start = 0\n",
    "target_altitude_space_end = 1\n",
    "target_altitude_space_num = 10\n",
    "target_altitude_space = np.linspace(target_altitude_space_start, target_altitude_space_end, target_altitude_space_num)\n",
    "\n",
    "runway_distance_space_start = -1\n",
    "runway_distance_space_end = 1\n",
    "runway_distance_space_num = 20\n",
    "runway_distance_space = np.linspace(runway_distance_space_start, runway_distance_space_end, runway_distance_space_num)\n",
    "\n",
    "\n",
    "#print(\"altitude_space:\", altitude_space)\n",
    "#print(\"vertical_velocity_space:\", vertical_velocity_space)\n",
    "#print(\"target_altitude_space:\", target_altitude_space)\n",
    "#print(\"runway_distance_space:\", runway_distance_space)\n",
    "\n",
    "\n",
    "#import gymnasium as gym\n",
    "#from descent_env import DescentEnv\n",
    "#import numpy as np\n",
    "#import random\n",
    "#\n",
    "#env = DescentEnv(render_mode=\"human\") \n",
    "#obs, info = env.reset()\n",
    "#\n",
    "#num_episodes = 5 \n",
    "#max_steps_per_episode = 500 \n",
    "#\n",
    "#all_altitudes = []\n",
    "#all_vertical_velocities = []\n",
    "#all_target_altitudes = []\n",
    "#all_runway_distances = []\n",
    "#\n",
    "#for episode in range(num_episodes):\n",
    "#    obs, info = env.reset()\n",
    "#    done = False\n",
    "#    steps = 0\n",
    "#    print(f\"\\n--- Episodio {episode + 1} ---\")\n",
    "#    while not done and steps < max_steps_per_episode:\n",
    "#        # Aquí generas una acción aleatoria válida para tu entorno\n",
    "#        actions = [-1.0, -0.5, 0.0, 0.5, 1.0] # Acciones discretas sugeridas en el notebook\n",
    "#        action = np.array([random.choice(actions)])\n",
    "#\n",
    "#        obs, reward, done, truncated, info = env.step(action)\n",
    "#\n",
    "#        # Recolectar datos\n",
    "#        all_altitudes.append(obs[\"altitude\"][0])\n",
    "#        all_vertical_velocities.append(obs[\"vz\"][0])\n",
    "#        all_target_altitudes.append(obs[\"target_altitude\"][0])\n",
    "#        all_runway_distances.append(obs[\"runway_distance\"][0])\n",
    "#        steps += 1\n",
    "#        \n",
    "#    env.render()\n",
    "#    env.close()\n",
    "#\n",
    "## Analizar los datos recolectados\n",
    "#print(\"\\n--- Análisis de rangos observados ---\")\n",
    "#print(f\"Altitud: Min={np.min(all_altitudes):.2f}, Max={np.max(all_altitudes):.2f}\")\n",
    "#print(f\"Velocidad Vertical: Min={np.min(all_vertical_velocities):.2f}, Max={np.max(all_vertical_velocities):.2f}\")\n",
    "#print(f\"Altitud Objetivo: Min={np.min(all_target_altitudes):.2f}, Max={np.max(all_target_altitudes):.2f}\")\n",
    "#print(f\"Distancia Pista: Min={np.min(all_runway_distances):.2f}, Max={np.max(all_runway_distances):.2f}\")\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = list(np.linspace(-1, 1, 10))\n",
    "agent = QLearningAgent(\n",
    "    altitude_space, \n",
    "    vertical_velocity_space, \n",
    "    target_altitude_space,\n",
    "    runway_distance_space,  \n",
    "    actions=actions,\n",
    "    env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 3000\n",
    "epsilon = 0.99\n",
    "gamma = 0.8\n",
    "alpha = 0.3\n",
    "rewards = agent.train_agent(env=env, episodes=episodes, epsilon=epsilon, gamma=gamma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el estado a partir de la observación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = np.mean(rewards)\n",
    "print(f\"Promedio de recompensas: {average_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.title('Recompensas por episodio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rewards = agent.test_agent(env, episodes=500)\n",
    "test_average_reward = np.mean(test_rewards)\n",
    "print(f\"Promedio de recompensas en test: {test_average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de wandb\n",
    "wandb.login(key=\"b1666b9050a5ade20a5130837a3c3c5ac2e39580\")\n",
    "wandb.init(project=\"descent_env_training\", name=\"training_run\")\n",
    "# Guardar hiperparámetros\n",
    "wandb.config.update({\n",
    "    \"epsilon\": epsilon,\n",
    "    \"gamma\": gamma,\n",
    "    \"alpha\": alpha,\n",
    "    \"episodes\": episodes,\n",
    "    \"altitude_space_start\": altitude_space_start,\n",
    "    \"altitude_space_end\": altitude_space_end,\n",
    "    \"altitude_space_num\": altitude_space_num,\n",
    "    \"vertical_velocity_space_start\": vertical_velocity_space_start,\n",
    "    \"vertical_velocity_space_end\": vertical_velocity_space_end,\n",
    "    \"vertical_velocity_space_num\": vertical_velocity_space_num,\n",
    "    \"target_altitude_space_start\": target_altitude_space_start,\n",
    "    \"target_altitude_space_end\": target_altitude_space_end,\n",
    "    \"target_altitude_space_num\": target_altitude_space_num,\n",
    "    \"runway_distance_space_start\": runway_distance_space_start,\n",
    "    \"runway_distance_space_end\": runway_distance_space_end,\n",
    "    \"runway_distance_space_num\": runway_distance_space_num\n",
    "})\n",
    "\n",
    "# Guardar discretización de los datos\n",
    "wandb.log({\n",
    "    \"test_average_reward\": test_average_reward\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Seleccionar una dimensión de la tabla Q para analizar (por ejemplo, altitud y velocidad vertical)\n",
    "q_table_slice = agent.q  # Ajusta los índices según las dimensiones de tu tabla Q\n",
    "\n",
    "# Crear el mapa de calor\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(q_table_slice, cmap=\"viridis\", annot=False)\n",
    "plt.xlabel(\"Acciones\")\n",
    "plt.ylabel(\"Estados\")\n",
    "plt.title(\"Mapa de calor de la tabla Q\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent and visualize its execution\n",
    "wrapper = VideoRecorderWrapper(env, filename='landing_execution.mp4', fps=3)\n",
    "human_env = DescentEnv(render_mode='human')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "descent-env-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
