{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Descent Env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from descent_env import DescentEnv\n",
    "from q_learning_agent import QLearningAgent\n",
    "from q_learning_stochastic_agent import QLearningStochasticAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar render_mode a rgb_array para entrenar/testear\n",
    "from env_recorder_wrapper import VideoRecorderWrapper \n",
    "import wandb\n",
    "env = DescentEnv(render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de los estados\n",
    "\n",
    "**Nota:** es importante que chequeen el espacio de observación y el espacio de acción del entorno. Los números usados son ejemplos y pueden no ser correctos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altitude_space = np.concatenate([\n",
    "    np.linspace(0, 0.2, 2, endpoint=False),\n",
    "    np.linspace(0.2, 0.8, 6),\n",
    "    np.linspace(0.8, 1.3, 3),\n",
    "    np.linspace(1.3, 2.3,3),\n",
    "    np.linspace(2.3, 2.5, 3)]) \n",
    "\n",
    "vertical_velocity_space = np.linspace(-2.7,2.5, 3)\n",
    "\n",
    "target_altitude_space = np.concatenate([\n",
    "    np.linspace(0,0.2, 2, endpoint=False),\n",
    "    np.linspace(0.2, 0.8, 12),\n",
    "    np.linspace(0.8, 0.9, 2, )])\n",
    "\n",
    "#runway_distance_space = np.concatenate([\n",
    "#    np.linspace(-0.6, -0.5, 3, endpoint=False),\n",
    "#    np.linspace(-0.5, 0.1, 8),\n",
    "#    np.linspace(0.1, 1, 3)])\n",
    "\n",
    "runway_distance_space = np.concatenate([\n",
    "    np.linspace(-0.3, -0.01, 3, endpoint=False),  # post-pista (negativos)\n",
    "    np.linspace(-0.01, 0.1, 6),                   # justo antes y sobre pista\n",
    "    np.linspace(0.1, 0.5, 4),                     # aproximación cercana\n",
    "    np.linspace(0.5, 1.2, 3)                      # aproximación lejana\n",
    "])\n",
    "#print(\"altitude_space:\", altitude_space)\n",
    "#print(\"vertical_velocity_space:\", vertical_velocity_space)\n",
    "#print(\"target_altitude_space:\", target_altitude_space)\n",
    "#print(\"runway_distance_space:\", runway_distance_space)\n",
    "\n",
    "\n",
    "#import gymnasium as gym\n",
    "#from descent_env import DescentEnv\n",
    "#import numpy as np\n",
    "#import random\n",
    "#\n",
    "#env = DescentEnv(render_mode=\"human\") \n",
    "#obs, info = env.reset()\n",
    "#\n",
    "#num_episodes = 5 \n",
    "#max_steps_per_episode = 500 \n",
    "#\n",
    "#all_altitudes = []\n",
    "#all_vertical_velocities = []\n",
    "#all_target_altitudes = []\n",
    "#all_runway_distances = []\n",
    "#\n",
    "#for episode in range(num_episodes):\n",
    "#    obs, info = env.reset()\n",
    "#    done = False\n",
    "#    steps = 0\n",
    "#    print(f\"\\n--- Episodio {episode + 1} ---\")\n",
    "#    while not done and steps < max_steps_per_episode:\n",
    "#        # Aquí generas una acción aleatoria válida para tu entorno\n",
    "#        actions = [-1.0, -0.5, 0.0, 0.5, 1.0] # Acciones discretas sugeridas en el notebook\n",
    "#        action = np.array([random.choice(actions)])\n",
    "#\n",
    "#        obs, reward, done, truncated, info = env.step(action)\n",
    "#\n",
    "#        # Recolectar datos\n",
    "#        all_altitudes.append(obs[\"altitude\"][0])\n",
    "#        all_vertical_velocities.append(obs[\"vz\"][0])\n",
    "#        all_target_altitudes.append(obs[\"target_altitude\"][0])\n",
    "#        all_runway_distances.append(obs[\"runway_distance\"][0])\n",
    "#        steps += 1\n",
    "#        \n",
    "#    env.render()\n",
    "#    env.close()\n",
    "#\n",
    "## Analizar los datos recolectados\n",
    "#print(\"\\n--- Análisis de rangos observados ---\")\n",
    "#print(f\"Altitud: Min={np.min(all_altitudes):.2f}, Max={np.max(all_altitudes):.2f}\")\n",
    "#print(f\"Velocidad Vertical: Min={np.min(all_vertical_velocities):.2f}, Max={np.max(all_vertical_velocities):.2f}\")\n",
    "#print(f\"Altitud Objetivo: Min={np.min(all_target_altitudes):.2f}, Max={np.max(all_target_altitudes):.2f}\")\n",
    "#print(f\"Distancia Pista: Min={np.min(all_runway_distances):.2f}, Max={np.max(all_runway_distances):.2f}\")\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = list(np.linspace(-1, 1, 10))\n",
    "agent = QLearningStochasticAgent(\n",
    "    altitude_space, \n",
    "    vertical_velocity_space, \n",
    "    target_altitude_space,\n",
    "    runway_distance_space,  \n",
    "    actions=actions,\n",
    "    env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 6000\n",
    "epsilon = 0.99\n",
    "gamma = 0.5\n",
    "alpha = 0.5\n",
    "rewards = agent.train_agent(env=env, episodes=episodes, epsilon=epsilon, gamma=gamma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el estado a partir de la observación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_q_mean_heatmap_general(q_table, action_values, state_spaces, target_dim):\n",
    "    \"\"\"\n",
    "    Calcula un mapa de calor de Q(s, a) promediado sobre las demás dimensiones.\n",
    "\n",
    "    Args:\n",
    "        q_table (np.ndarray): La tabla Q.\n",
    "        action_values (list): Lista de valores de las acciones.\n",
    "        state_spaces (list): Lista de espacios discretizados para cada dimensión (s1, s2, s3, s4).\n",
    "        target_dim (int): Índice de la dimensión principal (0 para s1, 1 para s2, etc.).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matriz de tamaño (len(state_spaces[target_dim]), len(action_values)) con los valores promedios.\n",
    "    \"\"\"\n",
    "    heatmap = np.zeros((len(state_spaces[target_dim]), len(action_values)))\n",
    "    other_dims = [i for i in range(len(state_spaces)) if i != target_dim]\n",
    "\n",
    "    for target_index in range(len(state_spaces[target_dim])):\n",
    "        for action_index in range(len(action_values)):\n",
    "            q_sum = 0\n",
    "            count = 0\n",
    "            for indices in np.ndindex(*[len(state_spaces[dim]) for dim in other_dims]):\n",
    "                full_index = [0] * len(state_spaces)\n",
    "                full_index[target_dim] = target_index\n",
    "                for i, dim in enumerate(other_dims):\n",
    "                    full_index[dim] = indices[i]\n",
    "                full_index.append(action_index)\n",
    "                q_sum += q_table[tuple(full_index)]\n",
    "                count += 1\n",
    "            heatmap[target_index, action_index] = q_sum / count\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def plot_q_mean_heatmap_general(heatmap, state_space, action_values, state_label, title):\n",
    "    \"\"\"\n",
    "    Grafica un mapa de calor para Q(s, a) promedio.\n",
    "\n",
    "    Args:\n",
    "        heatmap (np.ndarray): Matriz (s, a) con valores promedio.\n",
    "        state_space (list): Valores discretizados para la dimensión principal.\n",
    "        action_values (list): Lista de valores de las acciones.\n",
    "        state_label (str): Etiqueta para la dimensión principal (e.g., \"Altitud\", \"Velocidad Vertical\").\n",
    "        title (str): Título del gráfico.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    im = plt.imshow(heatmap, aspect='auto', cmap='viridis', origin='lower',\n",
    "                    extent=[action_values[0], action_values[-1], state_space[0], state_space[-1]])\n",
    "    plt.colorbar(im, label=\"Q-value promedio\")\n",
    "    plt.xlabel(\"Acciones\")\n",
    "    plt.ylabel(state_label)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Graficar mapas de calor para todas las dimensiones\n",
    "dimensions = [\"Altitud (s1)\", \"Velocidad Vertical (s2)\", \"Altitud Objetivo (s3)\", \"Distancia a la Pista (s4)\"]\n",
    "state_spaces = [altitude_space, vertical_velocity_space, target_altitude_space, runway_distance_space]\n",
    "\n",
    "for target_dim, dimension_label in enumerate(dimensions):\n",
    "    heatmap = calculate_q_mean_heatmap_general(\n",
    "        q_table=agent.q,\n",
    "        action_values=actions,\n",
    "        state_spaces=state_spaces,\n",
    "        target_dim=target_dim\n",
    "    )\n",
    "    plot_q_mean_heatmap_general(\n",
    "        heatmap=heatmap,\n",
    "        state_space=state_spaces[target_dim],\n",
    "        action_values=actions,\n",
    "        state_label=dimension_label,\n",
    "        title=f\"Mapa de calor de Q({dimension_label}, a) promediando el resto de los estados\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = np.mean(rewards)\n",
    "print(f\"Promedio de recompensas: {average_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.title('Recompensas por episodio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rewards = agent.test_agent(env, episodes=500)\n",
    "test_average_reward = np.mean(test_rewards)\n",
    "print(f\"Promedio de recompensas en test: {test_average_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Agrupar recompensas por intervalos de episodios\n",
    "interval = 100  # Número de episodios por grupo\n",
    "grouped_rewards = [rewards[i:i + interval] for i in range(0, len(rewards), interval)]\n",
    "\n",
    "# Crear el boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(grouped_rewards, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel('Grupo de episodios (x100)')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.title('Distribución de recompensas por grupo de episodios de TRAIN')\n",
    "plt.xticks(ticks=range(1, len(grouped_rewards) + 1), labels=[f'{i*interval}-{(i+1)*interval}' for i in range(len(grouped_rewards))], rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Agrupar recompensas de test por intervalos de episodios\n",
    "interval = 100  # Número de episodios por grupo\n",
    "grouped_test_rewards = [test_rewards[i:i + interval] for i in range(0, len(test_rewards), interval)]\n",
    "\n",
    "# Crear el boxplot para las recompensas de test\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(grouped_test_rewards, patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel('Grupo de episodios de test (x100)')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.title('Distribución de recompensas por grupo de episodios de TEST')\n",
    "plt.xticks(ticks=range(1, len(grouped_test_rewards) + 1), labels=[f'{i*interval}-{(i+1)*interval}' for i in range(len(grouped_test_rewards))], rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_env = DescentEnv(render_mode=\"human\")\n",
    "agent.test_agent(human_env, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"b1666b9050a5ade20a5130837a3c3c5ac2e39580\")\n",
    "wandb.init(project=\"descent_env_training\", name=\"training_run\")\n",
    "# Guardar hiperparámetros\n",
    "wandb.config.update({\n",
    "    \"epsilon\": epsilon,\n",
    "    \"gamma\": gamma,\n",
    "    \"alpha\": alpha,\n",
    "    \"episodes\": episodes,\n",
    "})\n",
    "\n",
    "# Guardar discretización de los datos\n",
    "wandb.log({\n",
    "    \"test_average_reward\": test_average_reward\n",
    "})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "descent-env-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
